{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c885c245-5826-4c19-b78c-9ca99c64e365",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Forest Inventory with Airborne LiDAR\n",
    "\n",
    "Through this practical, you will use metrics derived from LiDAR point clouds to estimate stem volume (m3 ha-1), basal area (m2 ha-1), and diameter at breast height (DBH; cm). You have been provided with 166 forest plots dominated by Sitka spruce plantations.\n",
    "\n",
    "## LiDAR Data Analysis\n",
    "\n",
    "Given the volume of LiDAR required to cover the full area of this forest, it would be too much for you to process as part of this tutorial. Therefore, the LiDAR point cloud analysis has already been applied but consists of the following steps, which were applied using the SPDLib software:\n",
    "\n",
    " 1. Import the LAS/LAZ files into the SPD file format.\n",
    " 2. Filter noise from the point cloud\n",
    " 3. Classify ground returns\n",
    " 4. Populate point height field using the classified ground returns.\n",
    " 5. Calculate the LiDAR metrics from the point cloud\n",
    "\n",
    "\n",
    "## Regression Analysis\n",
    "\n",
    "### Defining the Problem\n",
    "\n",
    "Airborne LiDAR is a popular tool for forest inventory applications as it provides direct, high resolution measurements of canopy height and cover. However, airborne LiDAR cannot directly measure several important forest biometrics such as stem diameter, basal area, wood volume and biomass. To indirectly derive these forest biometrics from LiDAR measurements requires regression analysis using an area-based approach (White et al., 2013) or a single-tree approach (Breidenbach et al., 2010; Yu et al., 2011). In the area-based approach, LiDAR measurements are spatially gridded and converted into a range of  metrics (height percentiles and density statistics) that quantify the vertical and horizontal distribution of laser returns within the forest canopy. The LiDAR metrics are then regressed against forest inventory measurements recorded at ground level in fixed-area plots. The final outcome of this procedure is a wall-to-wall map of estimated forest inventory attributes across the region surveyed by the airborne LiDAR.\n",
    "\n",
    "You have been provided with three files:\n",
    "\n",
    " 1. **Forest_Plot_Metrics.csv** – The field plots with associated LiDAR forest metrics.\n",
    " 2. **Forest_ALS_Metrics.kea** – An image 20x20 m pixels with the LiDAR metrics for the whole forest area to which the regression relationships developed on the field plots will be applied providing stem volume (m3 ha-1), basal area (m2 ha-1) and DBH (cm) for the whole forest area. Note, this image has been masked to just the forest area.\n",
    " 3. **Forest_ALS_Valid.kea** - A binary image for the metrics image specifying which pixels should be used for processing (i.e., forest pixels). \n",
    "\n",
    "Our research challenge is to generate accurate predictions for all three of these response variables by using the LiDAR metrics as predictor variables. This challenge could be solved by predicting the values individually for each response variable through multiple regression (multiple predictors, one response variable). However, such an approach would not preserve the covariance/correlations between the response variables, therefore we will also use multivariate regression to predict all three response variables simultaneously.\n",
    "\n",
    "### Assumptions underlying regression analysis\n",
    "\n",
    "Regardless of whether we are using parametric or non-parametric regression methods, there are several assumptions inherent to regression analysis:\n",
    " 1. There exists a non-random relationship between out predictor and response variable(s). For example, Ordinary Least Squares (OLS) linear regression assumes a constant monotonic relationship between our predictor(s) and response(s).\n",
    " 2. Multicollinearity – the regression model should only contain predictor variables that are independent or exogeneous (i.e., they are not a function of predictor variables already present within the model).\n",
    " 3. Homoskedasticity – the residuals/errors derived from a regression model should have constant variance.\n",
    " 4. Normality – the residuals/errors derived from a regression model should be normally distributed (Gaussian). With larger training datasets, this assumption becomes less problematic as the residuals will naturally approximate a Gaussian distribution due to the Central Limit Theorem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baea345-a861-4cf9-8311-0200eec74a5a",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1e81c17-6a9d-43f4-a3ee-abf025a6dfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import rsgislib.tools.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abbf9fe-2965-4373-b133-faa7ce517dbd",
   "metadata": {},
   "source": [
    "# 2. Read the input plot data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c4581ac-f7f9-4702-ba38-1ab52bdb97db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the CSV file as a Pandas data frame - the df variable.\n",
    "df = pandas.read_csv(\"../data/lidar/Forest_Plot_Metrics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87231390-a78c-48c2-b1a3-2056ace3a670",
   "metadata": {},
   "source": [
    "# 3. Get Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0046e8cd-567b-4550-ac61-6ae0734c7cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependent Variables:  ['Mean DBH', 'BA / ha', 'Vol / ha']\n",
      "Independent Variables:  ['N_Pulses', 'N_Returns', 'First_Return', 'Multi_Return', 'LPI', 'CDensity', 'FCI', 'LCI', 'GapFrac', 'VCC', 'hMean', 'qhMean', 'h25', 'h50', 'h60', 'h70', 'h75', 'h80', 'h90', 'h95', 'h99', 'hmax', 'IQR', 'Skew', 'Kurtosis', 'VDR', 'CanopyRR', 'L_mean', 'L_scale', 'L_skewness', 'L_kurtosis', 'L_variation', 'Closed_Vol', 'Open_Vol', 'Oligophotic_Vol', 'Euphotic_Vol', 'cvm_filled_vol', 'cvm_filled_prop', 'Closed_Prop', 'Open_Prop', 'Oligophotic_Prop', 'Euphotic_Prop', 'p_mean', 'p_scale', 'p_skewness', 'p_kurtosis', 'p_variation', 'chm_rumple', 'chm_ruggedness', 'chm_roughness', 'chm_vf', 'chm_vl', 'chm_vd', 'wv_peaks', 'wv_auc', 'wv_mid', 'wv_min', 'wv_max', 'wv_width', 'wv_prominence', 'wv_midmin', 'wv_midmax', 'wv_minmax', 'h25f', 'h50f', 'h60f', 'h70f', 'h75f', 'h80f', 'h85f', 'h90f', 'h95f', 'h99f', 'L_mean_f', 'L_scale_f', 'L_skewness_f', 'L_kurtosis_f', 'L_variation_f']\n"
     ]
    }
   ],
   "source": [
    "# Get a list of the columns within the df dataframe\n",
    "cols = list(df.columns)\n",
    "\n",
    "# Get the dependent response column names\n",
    "dep_vars = cols[3:6]\n",
    "print(\"Dependent Variables: \", dep_vars)\n",
    "\n",
    "# Get the indepedent predictor column names\n",
    "ind_vars = cols[6:]\n",
    "print(\"Independent Variables: \", ind_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f499ea2-a54e-421d-a2e2-2c5b4c28d49d",
   "metadata": {},
   "source": [
    "# 4. Plotting Variables\n",
    "\n",
    "A common first step in analysing our data is to visualise it, this can be really help in finding any problems our data and also getting an initial idea of any relationships which might exist within the data. \n",
    "\n",
    "To read the CSV file with the plot data we will use the pandas Python module (https://pandas.pydata.org), which is a module specifically designed for tabular data analysis and manipulation (i.e., the kind of analysis you might do in an Excel spreadsheet). Pandas can make tasks such as creating plots relatively easy. \n",
    "\n",
    "For our data, where there are 78 independent variables and 3 dependent variables, there are 234 plots to create. Doing this manually would be very time consuming so we will use loops to create those plots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a3ec87-eb78-4136-a380-5fbf75ea1863",
   "metadata": {},
   "source": [
    "## 4.1 Create Output Directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4aed88d-77bb-4a2f-9689-85a5a1a68b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output directory for the plots\n",
    "out_plots_dir = \"out_plots\"\n",
    "\n",
    "# Check that the output directory exists and create if not\n",
    "if not os.path.exists(out_plots_dir):\n",
    "    os.mkdir(out_plots_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858afc4c-3d62-409d-9f32-98afe00bbbf2",
   "metadata": {},
   "source": [
    "## 4.2 Iterative Through Variables and Create plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89416912-374c-4506-8b3c-1ac556196378",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pfb/miniconda3/envs/osgeo-env-v4-dev/lib/python3.11/site-packages/pandas/plotting/_matplotlib/core.py:513: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n"
     ]
    }
   ],
   "source": [
    "# Turn off interactive mode so plots don't get outputted to the notebook\n",
    "# just outputted to the output directory\n",
    "plt.ioff()\n",
    "\n",
    "# Loop through the dependent variables\n",
    "for dep_var in dep_vars:\n",
    "    # For the output file name create a more useable\n",
    "    # string to be used when creating the output file name.\n",
    "    out_file_name_base = \"\"\n",
    "    if dep_var == \"Mean DBH\":\n",
    "        out_file_name_base = \"dbh\"\n",
    "    elif dep_var == \"BA / ha\":\n",
    "        out_file_name_base = \"ba\"\n",
    "    elif dep_var == \"Vol / ha\":\n",
    "        out_file_name_base = \"vol\"\n",
    "\n",
    "    # Loop through the independent variables\n",
    "    for ind_var in ind_vars:\n",
    "        # Create a scatter plot the two variables.\n",
    "        df.plot.scatter(x=ind_var, y=dep_var)\n",
    "        # Create the output file name and path for the plot\n",
    "        out_plot_file = os.path.join(\n",
    "            out_plots_dir, \"{}_v_{}.png\".format(out_file_name_base, ind_var.lower())\n",
    "        )\n",
    "        # Save the plot to disk.\n",
    "        plt.savefig(out_plot_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15879c4-5b3d-4772-924e-23eef4888b1b",
   "metadata": {},
   "source": [
    "Have a look through those plots, which variables look like they might be useful for our application? For example, comparing the 95th percentiles for each of the dependent variables:\n",
    "\n",
    "![Basal Area v 95th Height Percentile](figures/ba_v_h95.png)\n",
    "![DbH v 95th Height Percentile](figures/dbh_v_h95.png)\n",
    "![Volume v 95th Height Percentile](figures/vol_v_h95.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346ef8bc-2252-4e7e-98b1-de0176df7bde",
   "metadata": {},
   "source": [
    "# 5. Pearson Correlation\n",
    "\n",
    "The Pearson correlation coefficient measures the strength of a linear relationship between two variables, assuming the variables to be normally distributed (Gaussian). We can calculate the Pearson correlation between each of our predictor and response variables by running the following code. \n",
    "\n",
    "*Note. The outputted correlations are saved as a CSV file, which can then be opened in which ever spreadsheet application (e.g., Excel) of your choice.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da4d5ced-dcde-4196-ba39-537920b2c97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dict for sorting the correlations for each dependent variable\n",
    "out_data = dict()\n",
    "# Loop through the dependent variables\n",
    "for dep_var in dep_vars:\n",
    "    # Create a list for sorting the output correlation values\n",
    "    corr_vals = list()\n",
    "    # Loop through the independent variables\n",
    "    for ind_var in ind_vars:\n",
    "        # add the correlation values to the list\n",
    "        corr_vals.append(df[dep_var].corr(df[ind_var], method=\"pearson\"))\n",
    "    # Add the list of correlation values to the dict.\n",
    "    out_data[dep_var] = corr_vals\n",
    "\n",
    "# Use the dict to create a new pandas dataframe with the index\n",
    "# (i.e., row names) defined with the independent variables.\n",
    "df_corr = pandas.DataFrame(data=out_data, index=ind_vars)\n",
    "# Save the dataframe to a CSV file.\n",
    "df_corr.to_csv(\"Forest_Plot_Correlation_Metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69c8b611-4b4f-4b83-b338-99597a996d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean DBH</th>\n",
       "      <th>BA / ha</th>\n",
       "      <th>Vol / ha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N_Pulses</th>\n",
       "      <td>0.049515</td>\n",
       "      <td>0.063979</td>\n",
       "      <td>0.043206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N_Returns</th>\n",
       "      <td>0.179326</td>\n",
       "      <td>0.221542</td>\n",
       "      <td>0.199755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>First_Return</th>\n",
       "      <td>-0.578153</td>\n",
       "      <td>-0.735066</td>\n",
       "      <td>-0.686424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multi_Return</th>\n",
       "      <td>0.578153</td>\n",
       "      <td>0.735066</td>\n",
       "      <td>0.686424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LPI</th>\n",
       "      <td>-0.303634</td>\n",
       "      <td>-0.602969</td>\n",
       "      <td>-0.525768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L_mean_f</th>\n",
       "      <td>0.893750</td>\n",
       "      <td>0.847581</td>\n",
       "      <td>0.918863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L_scale_f</th>\n",
       "      <td>0.755496</td>\n",
       "      <td>0.618877</td>\n",
       "      <td>0.596091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L_skewness_f</th>\n",
       "      <td>-0.677335</td>\n",
       "      <td>-0.806945</td>\n",
       "      <td>-0.721413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L_kurtosis_f</th>\n",
       "      <td>0.396643</td>\n",
       "      <td>0.502776</td>\n",
       "      <td>0.519443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L_variation_f</th>\n",
       "      <td>-0.443372</td>\n",
       "      <td>-0.562431</td>\n",
       "      <td>-0.649771</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Mean DBH   BA / ha  Vol / ha\n",
       "N_Pulses       0.049515  0.063979  0.043206\n",
       "N_Returns      0.179326  0.221542  0.199755\n",
       "First_Return  -0.578153 -0.735066 -0.686424\n",
       "Multi_Return   0.578153  0.735066  0.686424\n",
       "LPI           -0.303634 -0.602969 -0.525768\n",
       "...                 ...       ...       ...\n",
       "L_mean_f       0.893750  0.847581  0.918863\n",
       "L_scale_f      0.755496  0.618877  0.596091\n",
       "L_skewness_f  -0.677335 -0.806945 -0.721413\n",
       "L_kurtosis_f   0.396643  0.502776  0.519443\n",
       "L_variation_f -0.443372 -0.562431 -0.649771\n",
       "\n",
       "[78 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A subset of the data can be viewed in the notebook:\n",
    "df_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7600aabc-3bd7-4969-a5ff-1defce9e3925",
   "metadata": {},
   "source": [
    "## 5.1 Selecting Variables using Correlation\n",
    "\n",
    "We can see that several LiDAR predictors are weakly correlated (Pearson r < 0.7) with our response variables, therefore we must filter or sort the values to identify the most informative predictors. You can either do that within your spreadsheet application or you can use pandas to do this for you. The following code sort the correlation values for each of the dependent variables, printing the top and bottom 10 variables for each dependent variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d8ed2e5-16a0-4737-8ee7-509eda039779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 with positive correlation (Vol / ha)\n",
      "          Mean DBH   BA / ha  Vol / ha\n",
      "chm_vf    0.840862  0.889441  0.944067\n",
      "h25f      0.871631  0.843865  0.923543\n",
      "h25       0.875487  0.832307  0.919221\n",
      "L_mean_f  0.893750  0.847581  0.918863\n",
      "h50f      0.894526  0.851424  0.918302\n",
      "h50       0.897673  0.845485  0.918211\n",
      "L_mean    0.899481  0.840543  0.917130\n",
      "hMean     0.899481  0.840543  0.917130\n",
      "qhMean    0.902685  0.840829  0.913642\n",
      "h60       0.902154  0.844715  0.913641\n",
      "\n",
      "\n",
      "Top 10 with negative correlation (Vol / ha)\n",
      "               Mean DBH   BA / ha  Vol / ha\n",
      "VDR           -0.729955 -0.806508 -0.836858\n",
      "Skew          -0.715047 -0.863131 -0.800589\n",
      "L_skewness    -0.699228 -0.850528 -0.774383\n",
      "L_skewness_f  -0.677335 -0.806945 -0.721413\n",
      "First_Return  -0.578153 -0.735066 -0.686424\n",
      "Open_Prop     -0.542855 -0.770198 -0.652703\n",
      "L_variation_f -0.443372 -0.562431 -0.649771\n",
      "GapFrac       -0.494603 -0.720181 -0.623158\n",
      "L_variation   -0.356848 -0.410068 -0.533406\n",
      "LPI           -0.303634 -0.602969 -0.525768\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort by the volume correlation column\n",
    "df_corr.sort_values(\"Vol / ha\", ascending=False, inplace=True)\n",
    "\n",
    "# Print the 10 highest values\n",
    "print(\"Top 10 with positive correlation (Vol / ha)\")\n",
    "print(df_corr.head(10))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Sort by the volume correlation column\n",
    "df_corr.sort_values(\"Vol / ha\", ascending=True, inplace=True)\n",
    "\n",
    "# Print the 10 lowest values\n",
    "print(\"Top 10 with negative correlation (Vol / ha)\")\n",
    "print(df_corr.head(10))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb0462c1-9e9b-4797-b81c-23e42c87369e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 with positive correlation (BA / ha)\n",
      "                  Mean DBH   BA / ha  Vol / ha\n",
      "chm_vf            0.840862  0.889441  0.944067\n",
      "Oligophotic_Vol   0.828886  0.888714  0.913210\n",
      "cvm_filled_vol    0.806828  0.877162  0.876274\n",
      "Oligophotic_Prop  0.636937  0.873505  0.789017\n",
      "h50f              0.894526  0.851424  0.918302\n",
      "h60f              0.899793  0.848827  0.912498\n",
      "L_mean_f          0.893750  0.847581  0.918863\n",
      "h70f              0.902668  0.846543  0.907705\n",
      "h75f              0.903823  0.845506  0.905233\n",
      "h50               0.897673  0.845485  0.918211\n",
      "\n",
      "\n",
      "Top 10 with negative correlation (BA / ha)\n",
      "               Mean DBH   BA / ha  Vol / ha\n",
      "Skew          -0.715047 -0.863131 -0.800589\n",
      "L_skewness    -0.699228 -0.850528 -0.774383\n",
      "L_skewness_f  -0.677335 -0.806945 -0.721413\n",
      "VDR           -0.729955 -0.806508 -0.836858\n",
      "Open_Prop     -0.542855 -0.770198 -0.652703\n",
      "First_Return  -0.578153 -0.735066 -0.686424\n",
      "GapFrac       -0.494603 -0.720181 -0.623158\n",
      "LPI           -0.303634 -0.602969 -0.525768\n",
      "Open_Vol      -0.221204 -0.579842 -0.495162\n",
      "L_variation_f -0.443372 -0.562431 -0.649771\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort by the DBH correlation column\n",
    "df_corr.sort_values(\"BA / ha\", ascending=False, inplace=True)\n",
    "\n",
    "# Print the 10 highest values\n",
    "print(\"Top 10 with positive correlation (BA / ha)\")\n",
    "print(df_corr.head(10))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Sort by the DBH correlation column\n",
    "df_corr.sort_values(\"BA / ha\", ascending=True, inplace=True)\n",
    "\n",
    "# Print the 10 lowest values\n",
    "print(\"Top 10 with negative correlation (BA / ha)\")\n",
    "print(df_corr.head(10))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2eb98b32-1b41-4afd-8bad-75057c019514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 with positive correlation (Mean DBH)\n",
      "      Mean DBH   BA / ha  Vol / ha\n",
      "h90   0.907691  0.833391  0.892995\n",
      "h95   0.907625  0.827773  0.885551\n",
      "h80   0.906667  0.840074  0.902401\n",
      "h95f  0.906421  0.829111  0.884814\n",
      "h75   0.906088  0.841299  0.905467\n",
      "h90f  0.905427  0.834601  0.891714\n",
      "h70   0.905383  0.842373  0.907951\n",
      "h85f  0.905073  0.838910  0.896868\n",
      "h80f  0.904752  0.842593  0.901564\n",
      "h75f  0.903823  0.845506  0.905233\n",
      "\n",
      "\n",
      "Top 10 with negative correlation (Mean DBH)\n",
      "               Mean DBH   BA / ha  Vol / ha\n",
      "VDR           -0.729955 -0.806508 -0.836858\n",
      "Skew          -0.715047 -0.863131 -0.800589\n",
      "L_skewness    -0.699228 -0.850528 -0.774383\n",
      "L_skewness_f  -0.677335 -0.806945 -0.721413\n",
      "First_Return  -0.578153 -0.735066 -0.686424\n",
      "Open_Prop     -0.542855 -0.770198 -0.652703\n",
      "GapFrac       -0.494603 -0.720181 -0.623158\n",
      "wv_minmax     -0.487706 -0.518161 -0.498442\n",
      "L_variation_f -0.443372 -0.562431 -0.649771\n",
      "Euphotic_Prop -0.428465 -0.221944 -0.369181\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort by the DBH correlation column\n",
    "df_corr.sort_values(\"Mean DBH\", ascending=False, inplace=True)\n",
    "\n",
    "# Print the 10 highest values\n",
    "print(\"Top 10 with positive correlation (Mean DBH)\")\n",
    "print(df_corr.head(10))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Sort by the DBH correlation column\n",
    "df_corr.sort_values(\"Mean DBH\", ascending=True, inplace=True)\n",
    "\n",
    "# Print the 10 lowest values\n",
    "print(\"Top 10 with negative correlation (Mean DBH)\")\n",
    "print(df_corr.head(10))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77bd1b1-7d73-429b-8d84-8ffe6f70945a",
   "metadata": {},
   "source": [
    "From the ordered above, we can see that CHM volume (chm_vf) is the strongest linear predictor of stem volume (Pearson r = 0.95), however several other height metrics are also strongly correlated. If we choose to sort the Pearson correlation values by mean DBH, we get a different ordering of the most informative predictor variables.\n",
    "\n",
    "\n",
    "![Volume v chm volume](figures/vol_v_chm_vf.png)\n",
    "\n",
    "We can see from the scatter plot that the relationship between CHM volume and stem volume is approximately linear. However, there is evidence of heteroscedasticity; plots with higher CHM volumes exhibit greater variance in their stem volumes.\n",
    "\n",
    "Heteroscedasticity is also evident in the relationship between the 95th discrete-return height percentile and mean DBH:\n",
    "\n",
    "![DbH v 95th Height Percentile](figures/dbh_v_h95.png)\n",
    "\n",
    "\n",
    "This reflects the fact that trees stop growing in height once they reach maturity (~ 20 m), however stem girth continues to increase with age.\n",
    "\n",
    "From our initial data exploration, we have determined the following:\n",
    " 1. There is a non-random (approximately linear) relationship between some of our LiDAR metrics and the forest inventory measurements.\n",
    " 2. Some of the LiDAR metrics are weakly correlated with our response variables, therefore we shall have to omit these predictor variables from the regression model.\n",
    " 3. There is evidence for heteroscedasticity in the data which may influence prediction accuracy.\n",
    "\n",
    "*Heteroscedasticity is a hard word to pronounce, but it doesn't need to be a difficult concept to understand. Put simply, heteroscedasticity (also spelled heteroskedasticity) refers to the circumstance in which the variability of a variable is unequal across the range of values of a second variable that predicts it.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d43c25e-3d87-4fff-9cc3-b633fae7b7b8",
   "metadata": {},
   "source": [
    "# 6. Detecting Multicollinearity\n",
    "\n",
    "One of the assumptions underpinning regression analysis is that a model should only contain predictor variables that are independent or exogeneous (i.e., the predictors should not be a function of other predictor variables already present within the model).\n",
    "\n",
    "To measure multicollinearity between our predictor variables we have two options:\n",
    " 1. Calculate the Pearson correlation coefficient (we could then choose to selectively remove predictors with r >= 0.9).\n",
    " 2. Calculate the variance inflation factor (VIF) for each predictor (we could then selectively remove predictors with VIF scores >= 10).\n",
    "\n",
    "The Pearson correlation coefficient provides a bivariate measure of collinearity (i.e., quantifying which two environmental variables are correlated with each another).\n",
    "\n",
    "Conversely, VIF scores provide a more robust multivariate measure of collinearity (multicollinearity). Python code has been provided for you to calculate the VIF scores for each predictor variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f612e719-eadb-4ec8-a8d7-7226c6097259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating VIF for 78 predictors variables...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "N_Pulses         2.943701e+02\n",
       "N_Returns        3.950012e+02\n",
       "First_Return     0.000000e+00\n",
       "Multi_Return     0.000000e+00\n",
       "LPI              1.252028e+01\n",
       "                     ...     \n",
       "L_mean_f         1.562954e+06\n",
       "L_scale_f        3.651456e+04\n",
       "L_skewness_f     9.959568e+02\n",
       "L_kurtosis_f     1.255413e+02\n",
       "L_variation_f    8.271528e+02\n",
       "Name: VIF, Length: 78, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vifs_series = rsgislib.tools.stats.calc_pandas_vif(df, cols=ind_vars)\n",
    "vifs_series.to_csv(\"Forest_VIF_scores.csv\")\n",
    "vifs_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08433f2b-fe89-4e87-8897-8d46d3d35333",
   "metadata": {},
   "source": [
    "## 6.1 Sort the VIF values\n",
    "\n",
    "The code above exported the VIF scores into a comma-separated text file (Forest_VIF_scores.csv) that can be opened in your spreadsheet application or pandas as we did with the correlations. However, we can also sort them using pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a952566-a8d0-456a-8a9b-c38fdba16298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 25 VIF Scores (VIF)\n",
      "                      VIF\n",
      "chm_vf       3.617349e+13\n",
      "chm_vl       1.712395e+13\n",
      "chm_vd       8.034968e+12\n",
      "GapFrac      1.055731e+07\n",
      "CDensity     1.051947e+07\n",
      "L_mean_f     1.562954e+06\n",
      "qhMean       8.058562e+05\n",
      "Open_Prop    1.242767e+05\n",
      "h70f         1.241849e+05\n",
      "h75f         1.232850e+05\n",
      "h85f         1.114476e+05\n",
      "h60          1.104849e+05\n",
      "h80          1.083900e+05\n",
      "h70          1.058382e+05\n",
      "h80f         9.515071e+04\n",
      "h90          9.138985e+04\n",
      "h50          8.916934e+04\n",
      "h60f         8.027949e+04\n",
      "h50f         7.021731e+04\n",
      "h90f         6.349782e+04\n",
      "h95          5.746946e+04\n",
      "h99          3.965946e+04\n",
      "Closed_Prop  3.699494e+04\n",
      "L_scale_f    3.651456e+04\n",
      "h95f         3.531742e+04\n",
      "\n",
      "\n",
      "Lowest 25 VIF Scores (VIF)\n",
      "                        VIF\n",
      "wv_max             0.000000\n",
      "Multi_Return       0.000000\n",
      "First_Return       0.000000\n",
      "hMean              0.000000\n",
      "h25                0.000000\n",
      "IQR                0.000000\n",
      "h75                0.000000\n",
      "cvm_filled_prop    0.000000\n",
      "Oligophotic_Prop   0.000000\n",
      "wv_width           0.000000\n",
      "cvm_filled_vol     0.000000\n",
      "wv_min             0.000000\n",
      "Oligophotic_Vol    0.000000\n",
      "Euphotic_Vol       0.000000\n",
      "Euphotic_Prop      0.000000\n",
      "L_mean             0.000000\n",
      "wv_midmin          2.689774\n",
      "wv_midmax          5.052254\n",
      "chm_ruggedness     6.114693\n",
      "wv_peaks           7.652464\n",
      "wv_minmax          7.714053\n",
      "chm_rumple         8.403410\n",
      "LPI               12.520276\n",
      "chm_roughness     22.518633\n",
      "wv_prominence     27.107486\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create dataframe from series\n",
    "vifs_df = pandas.DataFrame({\"VIF\": vifs_series})\n",
    "\n",
    "# Sort by the VIF column\n",
    "vifs_df.sort_values(\"VIF\", ascending=False, inplace=True)\n",
    "\n",
    "# Print the 25 highest values\n",
    "print(\"Top 25 VIF Scores (VIF)\")\n",
    "print(vifs_df.head(25))\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# Sort by the VIF column\n",
    "vifs_df.sort_values(\"VIF\", ascending=True, inplace=True)\n",
    "\n",
    "# Print the 25 lowest values\n",
    "print(\"Lowest 25 VIF Scores (VIF)\")\n",
    "print(vifs_df.head(25))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f8749a-2023-46b0-a4b6-6955c1a68920",
   "metadata": {},
   "source": [
    "We can see from these results that many of the LiDAR metrics are highly correlated with each other since they have VIF scores well above 10. This indicates that we must select an uncorrelated subset of the predictor variables for the regression analysis. This leads on to the next notebook on feature selection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
