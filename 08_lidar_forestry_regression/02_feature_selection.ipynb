{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a109fdee-8637-4de0-ae27-cb65fe31c3de",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "Feature selection is the process of removing uninformative and/or redundant predictor variables from a predictive model. It is an important procedure when a model contains a large number of predictor variables. There are several reasons for performing feature selection:\n",
    "\n",
    " 1. Parsimonious models are easier to interpret,\n",
    " 2. Parsimonious models are more efficient during training and prediction,\n",
    " 3. Parsimonious models are less prone to overfitting (see bias-variance tradeoff),\n",
    " 4. Parsimonious models are less prone to multicollinearity.\n",
    "\n",
    "In supervised feature selection, predictor variables are manually removed from a regression model based on \"expert\" knowledge (i.e., a review of the academic literature). This approach becomes impractical when dealing with a large number of predictors. Moreover, the erroneous removal of useful predictor variables can cause omitted variable bias due to an increase in the unexplained variance.\n",
    "\n",
    "Alternatively, there are several automated methods of performing unsupervised feature selection:\n",
    "\n",
    " 1. Stepwise or recursive feature elimination. In backwards elimination, the least informative predictor variables are iteratively removed (one-variable-at-a-time) from the model until no further variables can be deleted without a statistically significant loss of accuracy.\n",
    " 2. Correlation-based feature agglomeration. This method groups predictor variables together based on their correlation. A subset of predictors is then chosen (e.g., one variable from each group) in order to reduce multicollinearity.\n",
    " 3. Variance-based feature selection. Predictor variables with limited variance are omitted from the model because they are deemed to be uninformative, whilst predictors with very high variance are omitted as they are deemed to be unreliable (i.e., prone to noise or measurement error).\n",
    " 4. Regularisation-based feature selection. Regression models with L1 regularisation (e.g., Lasso) assign coefficients of zero to uninformative predictors (effectively eliminating them from the model).\n",
    " 5. PLS Regression. This algorithm creates linear combinations of the predictor variables that are correlated to the response variable(s). The method is useful for predictor variables with strong multicollinearity since the predictors are linearly transformed into new uncorrelated features (i.e., feature engineering).\n",
    "\n",
    "*Note: Flom & Cassell (2007) caution against the use of a traditional stepwise feature selection approach. We will therefore follow their recommendations during this section of the practical.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba738c2-503e-4b5d-b354-419e690a50ae",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96537281-73ab-4910-9024-46fbb292ab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas\n",
    "import numpy\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import rsgislib.tools.stats\n",
    "import rsgislib.tools.utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03f212a-339d-4380-8351-587617caa10c",
   "metadata": {},
   "source": [
    "# 2. Read the input plot data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92e54ca4-ff49-45a3-8866-f5cc2afba7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the CSV file as a Pandas data frame - the df variable.\n",
    "df = pandas.read_csv('../data/lidar/Forest_Plot_Metrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad32421-e4af-44ab-84be-8169a874826b",
   "metadata": {},
   "source": [
    "# 3. Get Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "995baadd-266e-4ecc-9da0-e50bebfa4b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependent Variables:  ['Mean DBH', 'BA / ha', 'Vol / ha']\n",
      "Independent Variables:  ['N_Pulses', 'N_Returns', 'First_Return', 'Multi_Return', 'LPI', 'CDensity', 'FCI', 'LCI', 'GapFrac', 'VCC', 'hMean', 'qhMean', 'h25', 'h50', 'h60', 'h70', 'h75', 'h80', 'h90', 'h95', 'h99', 'hmax', 'IQR', 'Skew', 'Kurtosis', 'VDR', 'CanopyRR', 'L_mean', 'L_scale', 'L_skewness', 'L_kurtosis', 'L_variation', 'Closed_Vol', 'Open_Vol', 'Oligophotic_Vol', 'Euphotic_Vol', 'cvm_filled_vol', 'cvm_filled_prop', 'Closed_Prop', 'Open_Prop', 'Oligophotic_Prop', 'Euphotic_Prop', 'p_mean', 'p_scale', 'p_skewness', 'p_kurtosis', 'p_variation', 'chm_rumple', 'chm_ruggedness', 'chm_roughness', 'chm_vf', 'chm_vl', 'chm_vd', 'wv_peaks', 'wv_auc', 'wv_mid', 'wv_min', 'wv_max', 'wv_width', 'wv_prominence', 'wv_midmin', 'wv_midmax', 'wv_minmax', 'h25f', 'h50f', 'h60f', 'h70f', 'h75f', 'h80f', 'h85f', 'h90f', 'h95f', 'h99f', 'L_mean_f', 'L_scale_f', 'L_skewness_f', 'L_kurtosis_f', 'L_variation_f']\n"
     ]
    }
   ],
   "source": [
    "# Get a list of the columns within the df dataframe\n",
    "cols = list(df.columns)\n",
    "\n",
    "# Get the dependent response column names\n",
    "dep_vars = cols[3:6]\n",
    "print(\"Dependent Variables: \", dep_vars)\n",
    "\n",
    "# Get the indepedent predictor column names\n",
    "ind_vars = cols[6:]\n",
    "print(\"Independent Variables: \", ind_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8ade15-5506-4447-bef3-0a53a23a8f5b",
   "metadata": {},
   "source": [
    "# 4. Variance-based feature selection\n",
    "\n",
    "In this section, we demonstrate how to perform a variance-based feature selection to remove uninformative and unreliable predictor variables.\n",
    "\n",
    "The following python code will remove uninformative/unreliable predictors using the coefficient of quartile variation (CQV); a measure of dispersion based on the inter-quartile range. The CQV has two advantages over the default variance metric used in sklearn.feature_selection.VarianceThreshold():\n",
    "\n",
    " 1. it is a normalised metric (i.e. it is independent of feature scaling) therefore a single variance threshold can be applied to all of the predictor variables,\n",
    " 2. it is more robust to outliers than measures of dispersion based on the sample mean such as the coefficient of variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72308ceb-73cc-4709-85a1-541668a4e446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating CQV for 78 predictor variables...\n",
      "Median CQV: 0.3652239809806177\n",
      "Selected 50 useful predictors...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['LPI',\n",
       " 'GapFrac',\n",
       " 'hMean',\n",
       " 'qhMean',\n",
       " 'h25',\n",
       " 'h50',\n",
       " 'h60',\n",
       " 'h70',\n",
       " 'h75',\n",
       " 'h80',\n",
       " 'h90',\n",
       " 'h95',\n",
       " 'h99',\n",
       " 'hmax',\n",
       " 'IQR',\n",
       " 'L_mean',\n",
       " 'L_scale',\n",
       " 'Closed_Vol',\n",
       " 'Open_Vol',\n",
       " 'Oligophotic_Vol',\n",
       " 'cvm_filled_vol',\n",
       " 'Closed_Prop',\n",
       " 'Open_Prop',\n",
       " 'Oligophotic_Prop',\n",
       " 'p_mean',\n",
       " 'p_scale',\n",
       " 'p_skewness',\n",
       " 'p_kurtosis',\n",
       " 'chm_rumple',\n",
       " 'chm_ruggedness',\n",
       " 'chm_roughness',\n",
       " 'chm_vf',\n",
       " 'chm_vl',\n",
       " 'chm_vd',\n",
       " 'wv_auc',\n",
       " 'wv_mid',\n",
       " 'wv_width',\n",
       " 'wv_midmax',\n",
       " 'h25f',\n",
       " 'h50f',\n",
       " 'h60f',\n",
       " 'h70f',\n",
       " 'h75f',\n",
       " 'h80f',\n",
       " 'h85f',\n",
       " 'h90f',\n",
       " 'h95f',\n",
       " 'h99f',\n",
       " 'L_mean_f',\n",
       " 'L_scale_f']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get list of coefficient of quartile variation (CQV) good columns\n",
    "good_cols_names = rsgislib.tools.stats.cqv_threshold(df, ind_vars, lowthreshold=0.25, highthreshold=0.75)\n",
    "good_cols_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80128a58-5f1d-480c-b124-36a90544ede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the list of good columns to a text file.\n",
    "rsgislib.tools.utils.write_list_to_file(good_cols_names, './Forest_cqv_good_cols.txt')\n",
    "\n",
    "# Create the list of columns to be outputted\n",
    "out_cols = numpy.append(cols[:6], good_cols_names)\n",
    "\n",
    "# Subset the dataframe to the selected columns\n",
    "out_df = df[out_cols]\n",
    "\n",
    "# Save the subsetted dataframe to a CSV file.\n",
    "out_df.to_csv('Forest_Plot_Metrics_CQV_Sel.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60bfe6e-54de-436e-9b25-5696c511308b",
   "metadata": {},
   "source": [
    "A **disadvantage** of this method is that it requires user-defined thresholds. You will notice that 28 of the predictor variables have been excluded because their CQV values were either below the minimum threshold of 0.25 or above the maximum threshold of 0.75:\n",
    "\n",
    "The subset of predictor variables has been saved in “Donegal_Plot_Metrics_CQV_Sel.csv”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409ec6e3-208c-4aa9-bdcc-a684df9a36dd",
   "metadata": {},
   "source": [
    "# 5. Correlation-based feature selection\n",
    "\n",
    "In this section, we demonstrate how to perform a correlation-based feature selection to select an uncorrelated subset of the predictor variables through feature agglomeration. This algorithm is used to cluster predictor variables that are correlated with each other. We then choose only one predictor variable from each cluster to reduce multicollinearity whilst also reducing the dimensionality of our regression model.\n",
    "\n",
    "The following python code will cluster the predictor variables based on the Pearson correlation distance metric. The Silhouette coefficient (Rousseeuw, 1987) is used to find the optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14e10dc8-5ad7-4b9b-a1b7-acfd91e4f19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00,  9.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found optimal number of clusters: 7\n",
      "Silhouette Coefficient: 0.28649170339408003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['CanopyRR',\n",
       " 'chm_vf',\n",
       " 'wv_peaks',\n",
       " 'wv_prominence',\n",
       " 'Closed_Vol',\n",
       " 'N_Returns',\n",
       " 'wv_midmax']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the Correlation based feature selecting using clustering\n",
    "good_cols_names = rsgislib.tools.stats.corr_feature_selection(df, dep_vars, ind_vars, n_max_clusters=12)\n",
    "good_cols_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dfee085-4e97-463e-b745-30a2caa99072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the list of good columns to a text file.\n",
    "rsgislib.tools.utils.write_list_to_file(good_cols_names, './Forest_corr_good_cols.txt')\n",
    "\n",
    "# Create the list of columns to be outputted\n",
    "out_cols = numpy.append(cols[:6], good_cols_names)\n",
    "\n",
    "# Subset the dataframe to the selected columns\n",
    "out_df = df[out_cols]\n",
    "\n",
    "# Save the subsetted dataframe to a CSV file.\n",
    "out_df.to_csv('Forest_Plot_Metrics_Corr_Sel.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb2c766-11f2-473d-a6fa-ed64d6c5f930",
   "metadata": {},
   "source": [
    "For this particular dataset, an optimal number of 7 clusters has been identified with a Silhouette coefficient of 0.29. From each of the 7 clusters, one predictor variable is chosen – the predictor with the strongest Pearson correlation to our response variables.\n",
    "\n",
    "The subset of predictor variables has been saved in Forest_Plot_Metrics_Corr_Sel.csv.\n",
    "\n",
    "To verify that this approach has been successful, we can calculate the VIF scores for each predictor variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "197b440d-0cfb-46f2-abfe-057afcb7d460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CanopyRR', 'chm_vf', 'wv_peaks', 'wv_prominence', 'Closed_Vol', 'N_Returns', 'wv_midmax']\n",
      "Calculating VIF for 7 predictors variables...\n"
     ]
    }
   ],
   "source": [
    "sel_cols = list(out_df.columns)\n",
    "sel_ind_vars = sel_cols[6:]\n",
    "print(sel_ind_vars)\n",
    "\n",
    "vifs_series = rsgislib.tools.stats.calc_pandas_vif(out_df, sel_ind_vars)\n",
    "\n",
    "vifs_series.to_csv('Forest_VIF_scores_corr_sel.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481e84a8-d607-441b-b6be-baad259e8d53",
   "metadata": {},
   "source": [
    "We can then print the variables as a sorted list based on the VIF values. You can see that the values are number lower than the previous VIF scores, reducing the multicollinearity, with all below 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c4cd4d1-269f-4ff3-9ed6-8a5635ec148c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    VIF\n",
      "CanopyRR       5.339051\n",
      "chm_vf         4.391161\n",
      "wv_prominence  1.970861\n",
      "wv_peaks       1.410226\n",
      "Closed_Vol     1.349030\n",
      "N_Returns      1.191667\n",
      "wv_midmax      1.112316\n"
     ]
    }
   ],
   "source": [
    "# Create dataframe from series\n",
    "vifs_df = pandas.DataFrame({\"VIF\": vifs_series})\n",
    "\n",
    "# Sort by the VIF column\n",
    "vifs_df.sort_values('VIF', ascending=False, inplace=True)\n",
    "\n",
    "# Print the sorted dataframe\n",
    "print(vifs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a0baab-afa0-4e1e-b1d8-104071ffc170",
   "metadata": {},
   "source": [
    "# 7. Regularisation-based feature selection\n",
    "\n",
    "To undertake regularisation-based feature selection in Python, we will use the LassoLars regressor in Scikit-Learn. The Lasso (least absolute shrinkage and selection operator) regression algorithm is linear model that uses L1 regularisation to assign coefficients of zero to uninformative predictor variables (effectively eliminating them from the regression model). The LARS algorithm (Efron et al., 2004) provides a means of estimating which variables to include in the model, as well as their coefficients.\n",
    "\n",
    "The Lasso algorithm has one hyper-parameter that needs to be optimised – the alpha parameter which is a regularisation coefficient used to scale the Manhattan distance (L1 norm). The optimal alpha value is dataset dependent, therefore it needs to be tuned through a grid search. In scikit-learn, this can be achieved with sklearn.linear_model.LassoLarsIC() using either the Akaike Information Criterion (AIC) or the Bayes Information Criterion (BIC).\n",
    "\n",
    "To perform the feature selection procedure, execute the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "133ad508-3389-4298-9dbb-1b3490113cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using regularization parameter (alpha) for the Lasso estimator of: 0.461\n",
      "['Skew', 'CanopyRR', 'Oligophotic_Prop', 'Euphotic_Prop', 'p_mean', 'p_kurtosis', 'p_variation', 'chm_rumple', 'chm_ruggedness', 'chm_vf', 'chm_vl', 'wv_midmin']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pete/miniforge3/envs/osgeo-env-v3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Run the LassoLars based feature selecting using clustering\n",
    "# alpha defined to ensure compatiability with worksheet\n",
    "good_cols_names = rsgislib.tools.stats.lassolars_feature_selection(df, dep_vars, ind_vars, alpha_val=0.461)\n",
    "print(good_cols_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6f39265-1d30-408b-9012-a9874f61b2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the list of good columns to a text file.\n",
    "rsgislib.tools.utils.write_list_to_file(good_cols_names, './Forest_lassolars_good_cols.txt')\n",
    "\n",
    "# Create the list of columns to be outputted\n",
    "out_cols = numpy.append(cols[:6], good_cols_names)\n",
    "\n",
    "# Subset the dataframe to the selected columns\n",
    "out_df = df[out_cols]\n",
    "\n",
    "# Save the subsetted dataframe to a CSV file.\n",
    "out_df.to_csv('Forest_Plot_Metrics_LassoLars_Sel.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865ee72d-136e-492f-97e7-f2eae8f38f66",
   "metadata": {},
   "source": [
    "The subset of predictor variables has been saved in Forest_Plot_Metrics_LassoLars_Sel.csv.\n",
    "\n",
    "The Python code will find the optimal alpha value using the BIC – note this has been defined as 0.461 in the code to ensure the same result is returned as it is used later in the tutorial. The Lasso regressor is then fit using the optimal alpha value and predictor variables with the non-zero coefficients are selected whilst those with zero coefficients are omitted. This results in 12 features being selected:\n",
    "\n",
    " 1. Skew\n",
    " 2. CanopyRR\n",
    " 3. Oligophotic_Prop\n",
    " 4. Euphotic_Prop\n",
    " 5. p_mean\n",
    " 6. p_kurtosis\n",
    " 7. p_variation\n",
    " 8. chm_rumple\n",
    " 9. chm_ruggedness\n",
    " 10. chm_vf\n",
    " 11. chm_vl\n",
    " 12. wv_midmin\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
